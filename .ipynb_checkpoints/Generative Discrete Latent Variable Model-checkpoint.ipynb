{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Generative Discrete Latent Variable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import transforms, constraints\n",
    "from pyro.infer import config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.autoguide import initialization as mcmc_inits\n",
    "from pyro.infer import infer_discrete, MCMC, NUTS, HMC, Predictive\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))\n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        return Z, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gq_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    base_Q_c = dist.Gamma(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_Q_c = pyro.sample(\"lam_Q_c\", dist.TransformedDistribution(base_Q_c, [transforms.OrderedTransform()]))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))  \n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        Q = pyro.sample('Q', dist.Poisson(Vindex(lam_Q_c)[...,Z.long()]).mask(data[\"mask\"][\"Q\"]), obs=data[\"data\"][\"Q\"])\n",
    "\n",
    "        return Z, G, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G - Q spearmanr 0.8555847101426786\n",
      "G - Q pearsonr 0.8752201882063496\n"
     ]
    }
   ],
   "source": [
    "def generate_data(model, params, n_data = 2000):\n",
    "    \n",
    "    none_data = {\"G\": None,\n",
    "               \"Q\": None}\n",
    "    data_mask = {\"G\": torch.ones(n_data).bool(),\n",
    "                \"Q\": torch.ones(n_data).bool()}\n",
    "    none_data = {\"data\": none_data, \"mask\": data_mask}\n",
    "\n",
    "    model = poutine.condition(gq_model, true_params)\n",
    "    trace = poutine.trace(model).get_trace(none_data)\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    for n in trace.nodes.keys():\n",
    "        if n in [\"G\", \"Q\"]:\n",
    "            data[n] = trace.nodes[n][\"value\"]\n",
    "            \n",
    "    return {\"data\": data, \"mask\": data_mask}\n",
    "\n",
    "\n",
    "true_params = {\"lam_Q_c\": torch.tensor([[3.5, 14.0, 22., 28.0]]), \"lam_G_c\": torch.tensor([[1.0, 5.5, 12.0, 15.0]]), \"std_G_c\": torch.tensor([[1.0, 0.5, 1.0, 2.0]]), \"pi_Z_c\": torch.tensor([[0.25, 0.45, 0.15, 0.15]])}\n",
    "train_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "test_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "\n",
    "print(\"G - Q spearmanr\", spearmanr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])\n",
    "print(\"G - Q pearsonr\", pearsonr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lam_G_c': tensor([[ 1.0119,  5.6123, 10.1842, 14.5476]]),\n",
       " 'std_G_c': tensor([[1.0086, 0.9856, 0.9943, 0.9906]]),\n",
       " 'lam_Q_c': tensor([[ 1.0088, 14.6024, 21.3029, 29.6443]]),\n",
       " 'pi_Z_c': tensor([[0.2396, 0.2465, 0.2570, 0.2569]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Predictive(gq_model, num_samples = 2000)\n",
    "init_params = p(train_data)\n",
    "init_params = {k: torch.mean(v, dim = 0) for k,v in init_params.items() if k not in [\"Z\", \"G\", \"Q\"]}\n",
    "init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:34,  2.87it/s, step size=7.15e-02, acc. prob=0.963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      1.06      0.05      1.06      1.02      1.11     20.05      0.98\n",
      "lam_G_c[0,1]      5.51      0.02      5.51      5.49      5.53     91.62      0.98\n",
      "lam_G_c[0,2]     11.93      0.09     11.93     11.84     12.04     26.69      0.99\n",
      "lam_G_c[0,3]     14.90      0.16     14.88     14.75     15.04     32.41      1.03\n",
      "lam_Q_c[0,0]      3.46      0.09      3.46      3.36      3.55     33.77      0.98\n",
      "lam_Q_c[0,1]     13.81      0.12     13.81     13.67     13.90     70.58      0.98\n",
      "lam_Q_c[0,2]     21.55      0.38     21.57     21.15     22.00     25.65      1.00\n",
      "lam_Q_c[0,3]     28.18      0.30     28.12     27.81     28.35     38.77      0.99\n",
      " pi_Z_c[0,0]      0.26      0.01      0.26      0.25      0.27     48.79      0.98\n",
      " pi_Z_c[0,1]      0.43      0.01      0.43      0.42      0.44     43.14      0.98\n",
      " pi_Z_c[0,2]      0.14      0.01      0.14      0.12      0.15     34.13      1.01\n",
      " pi_Z_c[0,3]      0.17      0.01      0.17      0.16      0.18     22.44      1.01\n",
      "std_G_c[0,0]      1.04      0.04      1.04      1.00      1.09     63.20      1.03\n",
      "std_G_c[0,1]      0.50      0.01      0.50      0.50      0.51     49.18      1.00\n",
      "std_G_c[0,2]      0.96      0.07      0.96      0.93      1.07     15.54      1.01\n",
      "std_G_c[0,3]      2.16      0.10      2.16      2.03      2.24     41.80      1.01\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(gq_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "gq_mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "gq_mcmc.run(train_data)\n",
    "gq_mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation – predictive likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data, sites=[]):\n",
    "    \n",
    "    impute_data = copy.deepcopy(data)\n",
    "    for k in sites:\n",
    "        impute_data['data'][k] = None ## remove data site\n",
    "        impute_data['mask'][k] = torch.zeros(impute_data[\"mask\"][k].shape[0]).bool() ## set data mask to False\n",
    "    return impute_data\n",
    "\n",
    "\n",
    "def evaluate_point_predictons(pred_sites, gt_data):\n",
    "    pred_comp = dict()\n",
    "\n",
    "    for k in gt_data[\"data\"].keys():\n",
    "        if k in pred_sites.keys():\n",
    "            \n",
    "            hat_data = torch.mean(pred_sites[k], dim = 0)\n",
    "            \n",
    "            mse = mean_squared_error(gt_data[\"data\"][k].type(torch.float), hat_data.type(torch.float))\n",
    "            f1 = f1_score(gt_data[\"data\"][k].type(torch.int), hat_data.type(torch.int), average='weighted')\n",
    "            print(f\"{str(k)}: mse {mse}, weighted f1 {f1}\")\n",
    "\n",
    "\n",
    "def compute_exp_pred_lik(post_loglik):\n",
    "    \n",
    "    ### computes pointwise expected log predictive density at each data point\n",
    "    sample_mean_exp_n = torch.mean(torch.exp(post_loglik), 0)\n",
    "    exp_log_lik = torch.exp(torch.mean(torch.log(sample_mean_exp_n), axis=0))\n",
    "    #exp_log_density[k] = (post_loglik[k].logsumexp(0) - math.log(post_loglik[k].shape[0])).sum().item()\n",
    "    return exp_log_lik.item()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pred_lik(mcmc, model, data, sites = [\"G\", \"Q\"]):\n",
    "\n",
    "    ### computes predictive likelihood\n",
    "    params = mcmc.get_samples()\n",
    "    num_samples = list(params.values())[0].shape[0] \n",
    "    sample_plate = pyro.plate(\"samples\", num_samples, dim=-2)\n",
    "    \n",
    "    pred_sites = dict()\n",
    "    \n",
    "    for site in sites: ## loop through observed sites\n",
    "\n",
    "        ## infer P(Z | Q, T, params)______\n",
    "        infer_z_model = poutine.condition(model, params)\n",
    "        infer_z_model = sample_plate(infer_z_model)\n",
    "        infer_z_model = infer_discrete(infer_z_model, first_available_dim=-3, temperature=1)\n",
    "        \n",
    "        imputed_data = impute_data(data, sites=[site]) ## impute observed site\n",
    "        impute_trace = poutine.trace(infer_z_model).get_trace(imputed_data)\n",
    "        Z = impute_trace.nodes[\"Z\"][\"value\"]\n",
    "        Z_params = {\"Z\": Z, **params}\n",
    "\n",
    "        ## infer P(G | Z, params)______\n",
    "        infer_site_model = poutine.condition(model, Z_params)\n",
    "        infer_site_model = sample_plate(infer_site_model)\n",
    "        #infer_site_model = infer_discrete(infer_site_model, first_available_dim=-3, temperature=1)\n",
    "        trace = poutine.trace(infer_site_model).get_trace(test_data)\n",
    "        trace.compute_log_prob()\n",
    "        \n",
    "        exp_pred_lik = compute_exp_pred_lik(trace.nodes[site][\"log_prob\"])\n",
    "        print(f\"{site}: exp_pred_lik {exp_pred_lik}\")\n",
    "            \n",
    "        pred_sites[site] = impute_trace.nodes[site][\"value\"]\n",
    "        \n",
    "    return pred_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: exp_pred_lik 0.20046105980873108\n",
      "Q: exp_pred_lik 0.06829340755939484\n",
      "G: mse 5.477782726287842, weighted f1 0.29086947527957024\n",
      "Q: mse 16.79648208618164, weighted f1 0.06293327824811758\n"
     ]
    }
   ],
   "source": [
    "gq_pred_sites = evaluate_pred_lik(gq_mcmc, gq_model, test_data, sites = [\"G\", \"Q\"])\n",
    "evaluate_point_predictons(gq_pred_sites, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...now check model with G only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:21,  4.69it/s, step size=1.37e-01, acc. prob=0.948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      1.06      0.06      1.05      0.98      1.11     25.75      1.00\n",
      "lam_G_c[0,1]      5.50      0.02      5.50      5.48      5.52     45.16      1.03\n",
      "lam_G_c[0,2]     11.96      0.09     11.97     11.85     12.05     36.08      0.98\n",
      "lam_G_c[0,3]     15.08      0.24     15.03     14.80     15.24     18.29      0.98\n",
      " pi_Z_c[0,0]      0.26      0.01      0.25      0.24      0.26     33.83      0.98\n",
      " pi_Z_c[0,1]      0.43      0.01      0.43      0.42      0.44     61.86      0.98\n",
      " pi_Z_c[0,2]      0.15      0.02      0.15      0.13      0.16     21.97      0.99\n",
      " pi_Z_c[0,3]      0.16      0.02      0.16      0.15      0.18     24.46      0.99\n",
      "std_G_c[0,0]      1.02      0.03      1.03      0.98      1.05     40.69      0.98\n",
      "std_G_c[0,1]      0.50      0.01      0.50      0.49      0.52     33.62      1.01\n",
      "std_G_c[0,2]      0.99      0.07      0.99      0.91      1.06     18.17      1.01\n",
      "std_G_c[0,3]      2.08      0.14      2.08      1.95      2.23     24.42      0.98\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(g_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "g_mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "g_mcmc.run(train_data)\n",
    "g_mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: exp_pred_lik 0.08622390776872635\n",
      "G: mse 26.382484436035156, weighted f1 0.05317511756223247\n"
     ]
    }
   ],
   "source": [
    "g_pred_sites = evaluate_pred_lik(g_mcmc, g_model, test_data, sites = [\"G\"])\n",
    "evaluate_point_predictons(g_pred_sites, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldstein_venv",
   "language": "python",
   "name": "goldstein_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
