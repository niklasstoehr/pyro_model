{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Generative Discrete Latent Variable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import copy\n",
    "\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import transforms, constraints\n",
    "from pyro.infer import config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.autoguide import initialization as mcmc_inits\n",
    "from pyro.infer import infer_discrete, MCMC, NUTS, HMC, Predictive\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))\n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        return Z, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gq_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    base_Q_c = dist.Gamma(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_Q_c = pyro.sample(\"lam_Q_c\", dist.TransformedDistribution(base_Q_c, [transforms.OrderedTransform()]))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))  \n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        Q = pyro.sample('Q', dist.Poisson(Vindex(lam_Q_c)[...,Z.long()]).mask(data[\"mask\"][\"Q\"]), obs=data[\"data\"][\"Q\"])\n",
    "\n",
    "        return Z, G, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G - Q spearmanr 0.8441171014786846\n",
      "G - Q pearsonr 0.88024442581624\n"
     ]
    }
   ],
   "source": [
    "def generate_data(model, params, n_data = 2000):\n",
    "    \n",
    "    none_data = {\"G\": None,\n",
    "               \"Q\": None}\n",
    "    data_mask = {\"G\": torch.ones(n_data).bool(),\n",
    "                \"Q\": torch.ones(n_data).bool()}\n",
    "    none_data = {\"data\": none_data, \"mask\": data_mask}\n",
    "\n",
    "    model = poutine.condition(gq_model, true_params)\n",
    "    trace = poutine.trace(model).get_trace(none_data)\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    for n in trace.nodes.keys():\n",
    "        if n in [\"G\", \"Q\"]:\n",
    "            data[n] = trace.nodes[n][\"value\"]\n",
    "            \n",
    "    return {\"data\": data, \"mask\": data_mask}\n",
    "\n",
    "\n",
    "true_params = {\"lam_Q_c\": torch.tensor([[3.5, 14.0, 22., 28.0]]), \"lam_G_c\": torch.tensor([[1.0, 5.5, 12.0, 15.0]]), \"std_G_c\": torch.tensor([[1.0, 0.5, 1.0, 2.0]]), \"pi_Z_c\": torch.tensor([[0.25, 0.45, 0.15, 0.15]])}\n",
    "train_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "test_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "\n",
    "print(\"G - Q spearmanr\", spearmanr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])\n",
    "print(\"G - Q pearsonr\", pearsonr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lam_G_c': tensor([[ 0.9582,  5.6122,  9.9920, 14.3724]]),\n",
       " 'std_G_c': tensor([[1.0299, 1.0035, 0.9714, 0.9720]]),\n",
       " 'lam_Q_c': tensor([[ 1.0370, 49.8054, 57.0079, 65.8973]]),\n",
       " 'pi_Z_c': tensor([[0.2490, 0.2442, 0.2578, 0.2490]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Predictive(gq_model, num_samples = 2000)\n",
    "init_params = p(train_data)\n",
    "init_params = {k: torch.mean(v, dim = 0) for k,v in init_params.items() if k not in [\"Z\", \"G\", \"Q\"]}\n",
    "init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:33,  2.96it/s, step size=7.16e-02, acc. prob=0.926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      1.00      0.03      1.00      0.97      1.04     16.10      1.03\n",
      "lam_G_c[0,1]      5.52      0.01      5.52      5.50      5.53     37.27      0.99\n",
      "lam_G_c[0,2]     11.95      0.08     11.94     11.85     12.01     56.38      0.98\n",
      "lam_G_c[0,3]     15.19      0.17     15.21     15.00     15.37     37.83      1.01\n",
      "lam_Q_c[0,0]      3.59      0.11      3.58      3.51      3.73     45.82      1.03\n",
      "lam_Q_c[0,1]     13.96      0.15     13.95     13.87     14.22     42.03      1.00\n",
      "lam_Q_c[0,2]     22.08      0.36     22.09     21.66     22.44     49.34      1.03\n",
      "lam_Q_c[0,3]     27.97      0.39     27.97     27.70     28.53     75.57      1.00\n",
      " pi_Z_c[0,0]      0.25      0.01      0.25      0.24      0.26     62.20      0.99\n",
      " pi_Z_c[0,1]      0.46      0.01      0.46      0.45      0.47     64.91      1.01\n",
      " pi_Z_c[0,2]      0.15      0.01      0.15      0.14      0.16     33.47      1.01\n",
      " pi_Z_c[0,3]      0.14      0.01      0.14      0.13      0.15     48.86      0.98\n",
      "std_G_c[0,0]      0.98      0.04      0.98      0.94      1.02    117.42      0.98\n",
      "std_G_c[0,1]      0.49      0.01      0.49      0.47      0.50     86.49      1.01\n",
      "std_G_c[0,2]      0.96      0.04      0.96      0.92      1.01     28.98      0.99\n",
      "std_G_c[0,3]      1.77      0.10      1.77      1.65      1.85     46.20      0.99\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(gq_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "mcmc.run(train_data)\n",
    "mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation – predictive likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G exp_pred_lik: 0.06633149832487106\n",
      "Q exp_pred_lik: 0.008801787160336971\n"
     ]
    }
   ],
   "source": [
    "def impute_data(data, sites=[]):\n",
    "    \n",
    "    impute_data = copy.deepcopy(data)\n",
    "    for k in sites:\n",
    "        impute_data['data'][k] = None ## remove data site\n",
    "        impute_data['mask'][k] = torch.zeros(impute_data[\"mask\"][k].shape[0]).bool() ## set data mask to False\n",
    "    return impute_data\n",
    "\n",
    "\n",
    "\n",
    "def compute_exp_pred_lik(post_loglik):\n",
    "    \n",
    "    ### computes pointwise expected log predictive density at each data point\n",
    "    sample_mean_exp_n = torch.mean(torch.exp(post_loglik), 0)\n",
    "    exp_log_lik = torch.exp(torch.mean(torch.log(sample_mean_exp_n), axis=0))\n",
    "    #exp_log_density[k] = (post_loglik[k].logsumexp(0) - math.log(post_loglik[k].shape[0])).sum().item()\n",
    "    return exp_log_lik.item()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pred_lik(mcmc, model, data, sites = [\"G\", \"Q\"]):\n",
    "\n",
    "    ### computes predictive likelihood\n",
    "    params = mcmc.get_samples()\n",
    "    num_samples = list(params.values())[0].shape[0] \n",
    "    sample_plate = pyro.plate(\"samples\", num_samples, dim=-2)\n",
    "    \n",
    "    for site in sites: ## loop through observed sites\n",
    "\n",
    "        ## infer P(Z | Q, T, params)______\n",
    "        infer_z_model = poutine.condition(model, params)\n",
    "        infer_z_model = sample_plate(infer_z_model)\n",
    "        infer_z_model = infer_discrete(infer_z_model, first_available_dim=-3, temperature=1)\n",
    "        \n",
    "        imputed_data = impute_data(data, sites=[site]) ## impute observed site\n",
    "        trace = poutine.trace(infer_z_model).get_trace(imputed_data)\n",
    "        Z = trace.nodes[\"Z\"][\"value\"]\n",
    "        Z_params = {\"Z\": Z, **params}\n",
    "\n",
    "        ## infer P(G | Z, params)______\n",
    "        infer_site_model = poutine.condition(model, Z_params)\n",
    "        infer_site_model = sample_plate(infer_site_model)\n",
    "        cond_model = infer_discrete(infer_site_model, first_available_dim=-3, temperature=1)\n",
    "        trace = poutine.trace(infer_site_model).get_trace(test_data)\n",
    "        trace.compute_log_prob()\n",
    "        exp_pred_lik = compute_exp_pred_lik(trace.nodes[site][\"log_prob\"])\n",
    "        \n",
    "        print(f\"{site} exp_pred_lik: {exp_pred_lik}\")\n",
    "    return exp_pred_lik\n",
    "\n",
    "\n",
    "exp_pred_lik = evaluate_pred_lik(mcmc, gq_model, test_data, sites = [\"G\", \"Q\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...now check model with G only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:24,  4.16it/s, step size=1.62e-01, acc. prob=0.922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      1.01      0.05      1.01      0.95      1.06     34.54      1.02\n",
      "lam_G_c[0,1]      5.51      0.02      5.51      5.48      5.52     54.11      0.98\n",
      "lam_G_c[0,2]     12.01      0.10     12.01     11.88     12.08     19.55      1.06\n",
      "lam_G_c[0,3]     15.21      0.26     15.16     14.85     15.44     15.01      0.99\n",
      " pi_Z_c[0,0]      0.25      0.01      0.25      0.25      0.27     36.70      0.99\n",
      " pi_Z_c[0,1]      0.46      0.01      0.46      0.45      0.47     57.37      0.98\n",
      " pi_Z_c[0,2]      0.15      0.01      0.15      0.14      0.17     13.32      1.01\n",
      " pi_Z_c[0,3]      0.13      0.01      0.13      0.12      0.15     12.92      1.00\n",
      "std_G_c[0,0]      0.99      0.04      0.99      0.95      1.04     80.17      0.98\n",
      "std_G_c[0,1]      0.49      0.01      0.49      0.47      0.50     40.11      1.00\n",
      "std_G_c[0,2]      1.00      0.07      1.00      0.91      1.06     11.11      1.09\n",
      "std_G_c[0,3]      1.77      0.16      1.77      1.69      2.01     22.29      0.98\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(g_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "mcmc.run(train_data)\n",
    "mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G exp_pred_lik: 0.08961589634418488\n"
     ]
    }
   ],
   "source": [
    "exp_pred_lik = evaluate_pred_lik(mcmc, g_model, test_data, sites = [\"G\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldstein_venv",
   "language": "python",
   "name": "goldstein_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
