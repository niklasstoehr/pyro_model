{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Generative Discrete Latent Variable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import transforms, constraints\n",
    "from pyro.infer import config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.autoguide import initialization as mcmc_inits\n",
    "from pyro.infer import infer_discrete, MCMC, NUTS, HMC, Predictive\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))\n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        return Z, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gq_model(data=None):\n",
    "    \n",
    "    n_c = 4 ## number of latent classes\n",
    "\n",
    "    ## PRIOR\n",
    "    base_G_c = dist.Normal(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_G_c = pyro.sample(\"lam_G_c\", dist.TransformedDistribution(base_G_c, [transforms.OrderedTransform()]))\n",
    "    std_G_c = pyro.sample(\"std_G_c\", dist.Gamma(torch.ones(n_c), torch.ones(n_c)).to_event(1))\n",
    "\n",
    "    base_Q_c = dist.Gamma(torch.ones(n_c), torch.ones(n_c))\n",
    "    lam_Q_c = pyro.sample(\"lam_Q_c\", dist.TransformedDistribution(base_Q_c, [transforms.OrderedTransform()]))\n",
    "\n",
    "    pi_Z_c = pyro.sample(\"pi_Z_c\", dist.Dirichlet(torch.ones(n_c) / n_c))  \n",
    "\n",
    "    ## LIKELIHOOD\n",
    "    with pyro.plate('data_plate', data[\"mask\"][\"Q\"].shape[0]):\n",
    "\n",
    "        Z = pyro.sample('Z', dist.Categorical(pi_Z_c), infer={\"enumerate\": \"parallel\"})\n",
    "        G = pyro.sample('G', dist.Normal(Vindex(lam_G_c)[...,Z.long()], Vindex(std_G_c)[...,Z.long()]).mask(data[\"mask\"][\"G\"]),obs=data[\"data\"][\"G\"])\n",
    "        Q = pyro.sample('Q', dist.Poisson(Vindex(lam_Q_c)[...,Z.long()]).mask(data[\"mask\"][\"Q\"]), obs=data[\"data\"][\"Q\"])\n",
    "\n",
    "        return Z, G, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G - Q spearmanr 0.8427085529263129\n",
      "G - Q pearsonr 0.869439653917436\n"
     ]
    }
   ],
   "source": [
    "def generate_data(model, params, n_data = 2000):\n",
    "    \n",
    "    none_data = {\"G\": None,\n",
    "               \"Q\": None}\n",
    "    data_mask = {\"G\": torch.ones(n_data).bool(),\n",
    "                \"Q\": torch.ones(n_data).bool()}\n",
    "    none_data = {\"data\": none_data, \"mask\": data_mask}\n",
    "\n",
    "    model = poutine.condition(gq_model, true_params)\n",
    "    trace = poutine.trace(model).get_trace(none_data)\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    for n in trace.nodes.keys():\n",
    "        if n in [\"G\", \"Q\"]:\n",
    "            data[n] = trace.nodes[n][\"value\"]\n",
    "            \n",
    "    return {\"data\": data, \"mask\": data_mask}\n",
    "\n",
    "\n",
    "true_params = {\"lam_Q_c\": torch.tensor([[3.5, 14.0, 22., 28.0]]), \"lam_G_c\": torch.tensor([[1.0, 5.5, 12.0, 15.0]]), \"std_G_c\": torch.tensor([[1.0, 0.5, 1.0, 2.0]]), \"pi_Z_c\": torch.tensor([[0.25, 0.45, 0.15, 0.15]])}\n",
    "train_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "test_data = generate_data(gq_model, true_params, n_data = 2000)\n",
    "\n",
    "print(\"G - Q spearmanr\", spearmanr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])\n",
    "print(\"G - Q pearsonr\", pearsonr(train_data[\"data\"][\"G\"].numpy(), train_data[\"data\"][\"Q\"].numpy())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lam_G_c': tensor([[ 1.0146,  5.5856, 10.1043, 14.6680]]),\n",
       " 'std_G_c': tensor([[1.0145, 0.9698, 1.0560, 0.9983]]),\n",
       " 'lam_Q_c': tensor([[ 0.9730, 11.0158, 53.3710, 62.4217]]),\n",
       " 'pi_Z_c': tensor([[0.2501, 0.2467, 0.2575, 0.2457]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Predictive(gq_model, num_samples = 2000)\n",
    "init_params = p(train_data)\n",
    "init_params = {k: torch.mean(v, dim = 0) for k,v in init_params.items() if k not in [\"Z\", \"G\", \"Q\"]}\n",
    "init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:44,  2.22it/s, step size=6.16e-02, acc. prob=0.952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      0.91      0.04      0.91      0.86      0.94     57.52      1.00\n",
      "lam_G_c[0,1]      5.52      0.02      5.52      5.50      5.55     72.11      0.99\n",
      "lam_G_c[0,2]     11.88      0.09     11.87     11.75     11.94     44.90      1.03\n",
      "lam_G_c[0,3]     14.85      0.21     14.86     14.68     15.14     70.81      0.99\n",
      "lam_Q_c[0,0]      3.53      0.08      3.53      3.45      3.63     51.04      0.99\n",
      "lam_Q_c[0,1]     14.04      0.14     14.05     13.91     14.15     49.28      1.05\n",
      "lam_Q_c[0,2]     21.23      0.40     21.22     20.60     21.42     34.73      0.98\n",
      "lam_Q_c[0,3]     28.52      0.41     28.49     28.16     29.13     32.13      0.98\n",
      " pi_Z_c[0,0]      0.25      0.01      0.26      0.24      0.26     40.93      1.02\n",
      " pi_Z_c[0,1]      0.45      0.01      0.45      0.44      0.47     45.72      1.01\n",
      " pi_Z_c[0,2]      0.13      0.01      0.13      0.12      0.14     61.17      1.01\n",
      " pi_Z_c[0,3]      0.16      0.01      0.16      0.14      0.17     44.37      1.00\n",
      "std_G_c[0,0]      1.00      0.03      1.00      0.97      1.03     43.92      1.00\n",
      "std_G_c[0,1]      0.51      0.01      0.51      0.50      0.52     92.84      0.98\n",
      "std_G_c[0,2]      1.02      0.07      1.02      0.99      1.12     34.98      1.00\n",
      "std_G_c[0,3]      2.19      0.12      2.20      2.04      2.30     53.36      0.98\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(gq_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "mcmc.run(train_data)\n",
    "mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation – predictive likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: exp_pred_lik 0.0785723626613617\n",
      "Q: exp_pred_lik 0.012601005844771862\n",
      "G: mse 29.3521671295166, weighted f1 0.028663766659509187\n",
      "Q: mse 729.9135131835938, weighted f1 0.0017907551164431897\n"
     ]
    }
   ],
   "source": [
    "def impute_data(data, sites=[]):\n",
    "    \n",
    "    impute_data = copy.deepcopy(data)\n",
    "    for k in sites:\n",
    "        impute_data['data'][k] = None ## remove data site\n",
    "        impute_data['mask'][k] = torch.zeros(impute_data[\"mask\"][k].shape[0]).bool() ## set data mask to False\n",
    "    return impute_data\n",
    "\n",
    "\n",
    "def evaluate_point_predictons(pred_sites, gt_data):\n",
    "    pred_comp = dict()\n",
    "\n",
    "    for k in gt_data[\"data\"].keys():\n",
    "        if k in pred_sites.keys():\n",
    "            \n",
    "            hat_data = torch.mean(pred_sites[k], dim = 0)\n",
    "            \n",
    "            mse = mean_squared_error(gt_data[\"data\"][k].type(torch.float), hat_data.type(torch.float))\n",
    "            f1 = f1_score(gt_data[\"data\"][k].type(torch.int), hat_data.type(torch.int), average='weighted')\n",
    "            print(f\"{str(k)}: mse {mse}, weighted f1 {f1}\")\n",
    "\n",
    "\n",
    "def compute_exp_pred_lik(post_loglik):\n",
    "    \n",
    "    ### computes pointwise expected log predictive density at each data point\n",
    "    sample_mean_exp_n = torch.mean(torch.exp(post_loglik), 0)\n",
    "    exp_log_lik = torch.exp(torch.mean(torch.log(sample_mean_exp_n), axis=0))\n",
    "    #exp_log_density[k] = (post_loglik[k].logsumexp(0) - math.log(post_loglik[k].shape[0])).sum().item()\n",
    "    return exp_log_lik.item()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pred_lik(mcmc, model, data, sites = [\"G\", \"Q\"]):\n",
    "\n",
    "    ### computes predictive likelihood\n",
    "    params = mcmc.get_samples()\n",
    "    num_samples = list(params.values())[0].shape[0] \n",
    "    sample_plate = pyro.plate(\"samples\", num_samples, dim=-2)\n",
    "    \n",
    "    pred_sites = dict()\n",
    "    \n",
    "    for site in sites: ## loop through observed sites\n",
    "\n",
    "        ## infer P(Z | Q, T, params)______\n",
    "        infer_z_model = poutine.condition(model, params)\n",
    "        infer_z_model = sample_plate(infer_z_model)\n",
    "        infer_z_model = infer_discrete(infer_z_model, first_available_dim=-3, temperature=1)\n",
    "        \n",
    "        imputed_data = impute_data(data, sites=[site]) ## impute observed site\n",
    "        impute_trace = poutine.trace(infer_z_model).get_trace(imputed_data)\n",
    "        Z = impute_trace.nodes[\"Z\"][\"value\"]\n",
    "        Z_params = {\"Z\": Z, **params}\n",
    "\n",
    "        ## infer P(G | Z, params)______\n",
    "        infer_site_model = poutine.condition(model, Z_params)\n",
    "        infer_site_model = sample_plate(infer_site_model)\n",
    "        cond_model = infer_discrete(infer_site_model, first_available_dim=-3, temperature=1)\n",
    "        trace = poutine.trace(infer_site_model).get_trace(test_data)\n",
    "        trace.compute_log_prob()\n",
    "        \n",
    "        exp_pred_lik = compute_exp_pred_lik(trace.nodes[site][\"log_prob\"])\n",
    "        print(f\"{site}: exp_pred_lik {exp_pred_lik}\")\n",
    "            \n",
    "        pred_sites[site] = impute_trace.nodes[site][\"value\"]\n",
    "        \n",
    "    return pred_sites\n",
    "\n",
    "\n",
    "gq_pred_sites = evaluate_pred_lik(mcmc, gq_model, test_data, sites = [\"G\", \"Q\"])\n",
    "evaluate_point_predictons(gq_pred_sites, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...now check model with G only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██| 100/100 [00:18,  5.54it/s, step size=1.48e-01, acc. prob=0.922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median     12.5%     87.5%     n_eff     r_hat\n",
      "lam_G_c[0,0]      0.91      0.05      0.91      0.86      0.96     26.16      0.99\n",
      "lam_G_c[0,1]      5.52      0.02      5.52      5.51      5.54     47.66      1.02\n",
      "lam_G_c[0,2]     11.90      0.12     11.87     11.76     12.04     30.41      1.08\n",
      "lam_G_c[0,3]     14.71      0.37     14.65     14.31     15.07      3.50      1.85\n",
      " pi_Z_c[0,0]      0.25      0.01      0.25      0.24      0.26     33.48      0.98\n",
      " pi_Z_c[0,1]      0.46      0.01      0.45      0.44      0.46     41.71      0.98\n",
      " pi_Z_c[0,2]      0.12      0.02      0.11      0.09      0.14      3.12      1.95\n",
      " pi_Z_c[0,3]      0.17      0.02      0.17      0.15      0.20      3.29      1.85\n",
      "std_G_c[0,0]      1.00      0.04      1.01      0.98      1.07     36.38      0.98\n",
      "std_G_c[0,1]      0.52      0.01      0.52      0.50      0.53    172.69      0.98\n",
      "std_G_c[0,2]      1.01      0.11      0.99      0.83      1.08      5.60      1.34\n",
      "std_G_c[0,3]      2.25      0.15      2.23      2.15      2.47      6.45      1.40\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(g_model, init_strategy = mcmc_inits.init_to_value(values = init_params))\n",
    "mcmc = MCMC(nuts_kernel, num_samples= 50, warmup_steps= 50, num_chains=1)\n",
    "mcmc.run(train_data)\n",
    "mcmc.summary(prob=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: exp_pred_lik 0.08768624067306519\n",
      "G: mse 25.92630386352539, weighted f1 0.06497787063851514\n"
     ]
    }
   ],
   "source": [
    "g_pred_sites = evaluate_pred_lik(mcmc, g_model, test_data, sites = [\"G\"])\n",
    "evaluate_point_predictons(g_pred_sites, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldstein_venv",
   "language": "python",
   "name": "goldstein_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
